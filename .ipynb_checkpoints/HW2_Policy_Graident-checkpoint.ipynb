{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will solve a classic control problem - CartPole using policy gradient methods.\n",
    "\n",
    "First, you will implement the \"vanilla\" policy gradient method, i.e., a method that repeatedly computes **unbiased** estimates $\\hat{g}$ of $\\nabla_{\\theta} E[\\sum_t R_t]$ and takes gradient ascent steps $\\theta \\rightarrow \\theta + \\epsilon \\hat{g}$ so as to increase the total rewards collected in each episode. To make sure our code can solve multiple MDPs with different policy parameterizations, provided code follows an OOP manner and represents MDP and Policy as classes.\n",
    "\n",
    "The following code constructs an instance of the MDP, and then prints its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pgtf.baselines.linear_feature_baseline",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e050c800ec1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolicy_gradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpgtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_feature_baseline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearFeatureBaseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pgtf.baselines.linear_feature_baseline"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from policy_gradient import util\n",
    "from policy_gradient.policy import CategoricalPolicy\n",
    "from policy_gradient.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# CartPole-v0 is a MDP with finite state and action space. \n",
    "# In this environment, A pendulum is attached by an un-actuated joint to a cart, \n",
    "# and the goal is to prevent it from falling over. You can apply a force of +1 or -1 to the cart.\n",
    "# A reward of +1 is provided for every timestep that the pendulum remains upright. \n",
    "# To visualize CartPole-v0, please see https://gym.openai.com/envs/CartPole-v0\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Construct a neural network to represent policy which maps observed state to action. \n",
    "in_dim = util.flatten_space(env.observation_space)\n",
    "out_dim = util.flatten_space(env.action_space)\n",
    "hidden_dim = 8\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "policy = CategoricalPolicy(in_dim, out_dim, hidden_dim, opt, sess)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    def __init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "        discount_rate=.99):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.baseline = baseline\n",
    "        self.env = env\n",
    "        self.n_iter = n_iter\n",
    "        self.n_episode = n_episode\n",
    "        self.path_length = path_length\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "    def sample_path(self):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = self.env.reset()\n",
    "\n",
    "        for _ in range(self.path_length):\n",
    "            a = self.policy.act(ob.reshape(1, -1))\n",
    "            next_ob, r, done, _ = self.env.step(a)\n",
    "            obs.append(ob)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            ob = next_ob\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return dict(\n",
    "            observations=np.array(obs),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "        )\n",
    "\n",
    "    def process_paths(self, paths):\n",
    "        for p in paths:\n",
    "            if self.baseline != None:\n",
    "                b = self.baseline.predict(p)\n",
    "            else:\n",
    "                b = 0\n",
    "\n",
    "            r = util.discount_cumsum(p[\"rewards\"], self.discount_rate)\n",
    "            a = r - b\n",
    "\n",
    "            p[\"returns\"] = r\n",
    "            p[\"baselines\"] = b\n",
    "            p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8) # normalize\n",
    "            # TODO: Use the following line to compute advantage and compare\n",
    "            # how we can reduce variance by adding baseline and why it helps\n",
    "\n",
    "            #p[\"advantages\"] = a\n",
    "\n",
    "        #print(\"Rewards variance: {}\".format(np.var(p[\"returns\"])))\n",
    "        #print(\"Advantages variance: {}\".format(np.var(p[\"advantages\"])))\n",
    "\n",
    "        obs = np.concatenate([ p[\"observations\"] for p in paths ])\n",
    "        actions = np.concatenate([ p[\"actions\"] for p in paths ])\n",
    "        rewards = np.concatenate([ p[\"rewards\"] for p in paths ])\n",
    "        advantages = np.concatenate([ p[\"advantages\"] for p in paths ])\n",
    "\n",
    "        return dict(\n",
    "            observations=obs,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            advantages=advantages,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            paths = []\n",
    "            for _ in range(self.n_episode):\n",
    "                paths.append(self.sample_path())\n",
    "            data = self.process_paths(paths)\n",
    "            loss = self.policy.train(data[\"observations\"], data[\"actions\"], data[\"advantages\"])\n",
    "            avg_return = np.mean([sum(p[\"rewards\"]) for p in paths])\n",
    "            print(\"Iteration {}: Average Return = {}\".format(i, avg_return))\n",
    "            \n",
    "            # CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
    "            if avg_return >= 195:\n",
    "                print(\"Solve at {} iterations, which equals {} episodes.\".format(i, i*100))\n",
    "                break\n",
    "\n",
    "            if self.baseline != None:\n",
    "                self.baseline.fit(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearFeatureBaseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-362f75ac7fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdiscount_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFeatureBaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearFeatureBaseline' is not defined"
     ]
    }
   ],
   "source": [
    "n_iter = 200\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "baseline = LinearFeatureBaseline(env.spec)\n",
    "\n",
    "po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "po.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
