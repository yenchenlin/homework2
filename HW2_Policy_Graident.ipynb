{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will solve a classic control problem - CartPole using policy gradient methods.\n",
    "\n",
    "First, you will implement the \"vanilla\" policy gradient method, i.e., a method that repeatedly computes **unbiased** estimates $\\hat{g}$ of $\\nabla_{\\theta} E[\\sum_t R_t]$ and takes gradient ascent steps $\\theta \\rightarrow \\theta + \\epsilon \\hat{g}$ so as to increase the total rewards collected in each episode. To make sure our code can solve multiple MDPs with different policy parameterizations, provided code follows an OOP manner and represents MDP and Policy as classes.\n",
    "\n",
    "The following code constructs an instance of the MDP, and then prints its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2016-10-02 21:40:03,941] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from policy_gradient import util\n",
    "from policy_gradient.policy import CategoricalPolicy\n",
    "from policy_gradient.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# CartPole-v0 is a MDP with finite state and action space. \n",
    "# In this environment, A pendulum is attached by an un-actuated joint to a cart, \n",
    "# and the goal is to prevent it from falling over. You can apply a force of +1 or -1 to the cart.\n",
    "# A reward of +1 is provided for every timestep that the pendulum remains upright. \n",
    "# To visualize CartPole-v0, please see https://gym.openai.com/envs/CartPole-v0\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: construct a neural network to represent policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Construct a neural network to represent policy which maps observed state to action. \n",
    "in_dim = util.flatten_space(env.observation_space)\n",
    "out_dim = util.flatten_space(env.action_space)\n",
    "hidden_dim = 8\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "policy = CategoricalPolicy(in_dim, out_dim, hidden_dim, opt, sess)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: implement policy gradient computationÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    def __init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "        discount_rate=.99):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.baseline = baseline\n",
    "        self.env = env\n",
    "        self.n_iter = n_iter\n",
    "        self.n_episode = n_episode\n",
    "        self.path_length = path_length\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "    def sample_path(self):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = self.env.reset()\n",
    "\n",
    "        for _ in range(self.path_length):\n",
    "            a = self.policy.act(ob.reshape(1, -1))\n",
    "            next_ob, r, done, _ = self.env.step(a)\n",
    "            obs.append(ob)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            ob = next_ob\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return dict(\n",
    "            observations=np.array(obs),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "        )\n",
    "\n",
    "    def process_paths(self, paths):\n",
    "        for p in paths:\n",
    "            if self.baseline != None:\n",
    "                b = self.baseline.predict(p)\n",
    "            else:\n",
    "                b = 0\n",
    "\n",
    "            r = util.discount_cumsum(p[\"rewards\"], self.discount_rate)\n",
    "            a = r - b\n",
    "\n",
    "            p[\"returns\"] = r\n",
    "            p[\"baselines\"] = b\n",
    "            p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8) # normalize\n",
    "            # TODO: Use the following line to compute advantage and compare\n",
    "            # how we can reduce variance by adding baseline and why it helps\n",
    "\n",
    "            #p[\"advantages\"] = a\n",
    "\n",
    "        #print(\"Rewards variance: {}\".format(np.var(p[\"returns\"])))\n",
    "        #print(\"Advantages variance: {}\".format(np.var(p[\"advantages\"])))\n",
    "\n",
    "        obs = np.concatenate([ p[\"observations\"] for p in paths ])\n",
    "        actions = np.concatenate([ p[\"actions\"] for p in paths ])\n",
    "        rewards = np.concatenate([ p[\"rewards\"] for p in paths ])\n",
    "        advantages = np.concatenate([ p[\"advantages\"] for p in paths ])\n",
    "\n",
    "        return dict(\n",
    "            observations=obs,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            advantages=advantages,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            paths = []\n",
    "            for _ in range(self.n_episode):\n",
    "                paths.append(self.sample_path())\n",
    "            data = self.process_paths(paths)\n",
    "            loss = self.policy.train(data[\"observations\"], data[\"actions\"], data[\"advantages\"])\n",
    "            avg_return = np.mean([sum(p[\"rewards\"]) for p in paths])\n",
    "            print(\"Iteration {}: Average Return = {}\".format(i, avg_return))\n",
    "            \n",
    "            # CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
    "            if avg_return >= 195:\n",
    "                print(\"Solve at {} iterations, which equals {} episodes.\".format(i, i*100))\n",
    "                break\n",
    "\n",
    "            if self.baseline != None:\n",
    "                self.baseline.fit(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Average Return = 29.2\n",
      "Iteration 2: Average Return = 33.91\n",
      "Iteration 3: Average Return = 30.96\n",
      "Iteration 4: Average Return = 36.51\n",
      "Iteration 5: Average Return = 36.93\n",
      "Iteration 6: Average Return = 39.19\n",
      "Iteration 7: Average Return = 35.45\n",
      "Iteration 8: Average Return = 37.69\n",
      "Iteration 9: Average Return = 40.46\n",
      "Iteration 10: Average Return = 43.58\n",
      "Iteration 11: Average Return = 41.57\n",
      "Iteration 12: Average Return = 48.54\n",
      "Iteration 13: Average Return = 44.71\n",
      "Iteration 14: Average Return = 44.71\n",
      "Iteration 15: Average Return = 43.5\n",
      "Iteration 16: Average Return = 46.52\n",
      "Iteration 17: Average Return = 48.36\n",
      "Iteration 18: Average Return = 52.09\n",
      "Iteration 19: Average Return = 53.75\n",
      "Iteration 20: Average Return = 51.94\n",
      "Iteration 21: Average Return = 50.26\n",
      "Iteration 22: Average Return = 52.48\n",
      "Iteration 23: Average Return = 51.79\n",
      "Iteration 24: Average Return = 55.16\n",
      "Iteration 25: Average Return = 52.25\n",
      "Iteration 26: Average Return = 56.65\n",
      "Iteration 27: Average Return = 56.35\n",
      "Iteration 28: Average Return = 59.65\n",
      "Iteration 29: Average Return = 58.91\n",
      "Iteration 30: Average Return = 62.74\n",
      "Iteration 31: Average Return = 64.73\n",
      "Iteration 32: Average Return = 63.41\n",
      "Iteration 33: Average Return = 66.38\n",
      "Iteration 34: Average Return = 66.96\n",
      "Iteration 35: Average Return = 70.48\n",
      "Iteration 36: Average Return = 70.56\n",
      "Iteration 37: Average Return = 72.41\n",
      "Iteration 38: Average Return = 73.55\n",
      "Iteration 39: Average Return = 75.27\n",
      "Iteration 40: Average Return = 79.73\n",
      "Iteration 41: Average Return = 78.12\n",
      "Iteration 42: Average Return = 82.9\n",
      "Iteration 43: Average Return = 86.11\n",
      "Iteration 44: Average Return = 97.48\n",
      "Iteration 45: Average Return = 103.32\n",
      "Iteration 46: Average Return = 111.34\n",
      "Iteration 47: Average Return = 114.81\n",
      "Iteration 48: Average Return = 116.77\n",
      "Iteration 49: Average Return = 141.73\n",
      "Iteration 50: Average Return = 142.43\n",
      "Iteration 51: Average Return = 159.85\n",
      "Iteration 52: Average Return = 147.86\n",
      "Iteration 53: Average Return = 154.85\n",
      "Iteration 54: Average Return = 158.03\n",
      "Iteration 55: Average Return = 167.06\n",
      "Iteration 56: Average Return = 163.25\n",
      "Iteration 57: Average Return = 167.21\n",
      "Iteration 58: Average Return = 171.98\n",
      "Iteration 59: Average Return = 176.45\n",
      "Iteration 60: Average Return = 182.27\n",
      "Iteration 61: Average Return = 184.92\n",
      "Iteration 62: Average Return = 190.31\n",
      "Iteration 63: Average Return = 186.69\n",
      "Iteration 64: Average Return = 185.96\n",
      "Iteration 65: Average Return = 186.57\n",
      "Iteration 66: Average Return = 185.52\n",
      "Iteration 67: Average Return = 189.84\n",
      "Iteration 68: Average Return = 188.86\n",
      "Iteration 69: Average Return = 189.92\n",
      "Iteration 70: Average Return = 192.41\n",
      "Iteration 71: Average Return = 194.87\n",
      "Iteration 72: Average Return = 190.54\n",
      "Iteration 73: Average Return = 193.36\n",
      "Iteration 74: Average Return = 193.9\n",
      "Iteration 75: Average Return = 196.75\n",
      "Solve at 75 iterations, which equals 7500 episodes.\n"
     ]
    }
   ],
   "source": [
    "n_iter = 200\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "baseline = LinearFeatureBaseline(env.spec)\n",
    "\n",
    "po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "po.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Problem 1: implement baseline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
